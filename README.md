# DENSE
### (**D**ense **E**xpression of **N**uanced **S**emantic **E**ncoding)

## Overview
RL finetuning Gemma 3 4b to convert given text into dense text with higher per-word entropy.

## Background
I wanted to learn about LLM finetuning using reinforcement learning. I also love learning about words, meaning, and etymology, so this is a fun intersection of those interests. This is my first time finetuning an LLM, and I'm using the [trl](https://github.com/huggingface/trl) library's [Online DPO Trainer](https://huggingface.co/docs/trl/main/en/online_dpo_trainer).

## Model Info
I wanted to use something besides Llama, so I opted for [Gemma 3](https://huggingface.co/google/gemma-3-4b-it); more specifically, the instruction-tuned, 4b version.

## Data
I used various LLMs to generate synthetic data in the form of text chunks between 1 and 5 sentences. It wasn't a perfect process, but it's a fine place to start. Some issues I ran into during the synthetic data generation process:
- The LLMs tended to start out following my data generation prompt but quickly resorted to only generating 1-sentence chunks.
- The tenses and sentence structures often became too similar among the generated chunks.
- Topics and themes often became too similar among the generated chunks, and the writing styles were often too similar as well.
- Although my prompt asked for 1000 samples, the LLMs often generated only a few hundred (due to output length limits or issues with prompt-following). This also means that the number of samples are not even among the 4 LLMs used. Here are the number of samples generated by each LLM:
    - Google Gemini 2.5 Pro 05-06: 700
    - OpenAI GPT 4o: 123
    - Anthropic Claude Sonnet 4: 670
    - xAI Grok 3 Beta: 192

